{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Conversion GPU Testing - Simplified Version\n",
    "\n",
    "**Focus**: Test the 3 most practical models\n",
    "1. Seed-VC (easiest to setup)\n",
    "2. RVC (most popular)\n",
    "3. GPT-SoVITS (best quality)\n",
    "\n",
    "**Time**: ~2 hours\n",
    "**Cost**: FREE\n",
    "\n",
    "## ‚ö†Ô∏è BEFORE STARTING:\n",
    "1. Click **Runtime** ‚Üí **Change runtime type**\n",
    "2. Select **GPU** (T4)\n",
    "3. Click **Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Verify GPU (MUST show CUDA: True)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"\\n‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU! Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")\n",
    "    raise Exception(\"GPU not enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Install common dependencies\n",
    "!pip install -q librosa soundfile numpy matplotlib tqdm GPUtil\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Create test audio\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "os.makedirs('test_audio', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "def gen_voice(f0, duration, path):\n",
    "    sr = 16000\n",
    "    t = np.linspace(0, duration, int(sr * duration))\n",
    "    signal = sum(a * np.sin(2*np.pi*f0*i*t) for i, a in enumerate([1, 0.5, 0.3, 0.2], 1))\n",
    "    signal = signal / np.max(np.abs(signal)) * 0.7\n",
    "    sf.write(path, signal, sr)\n",
    "\n",
    "gen_voice(120, 3, 'test_audio/male.wav')\n",
    "gen_voice(220, 3, 'test_audio/female.wav')\n",
    "print(\"‚úÖ Test audio created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 1: Seed-VC (Recommended - Easiest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Seed-VC\n",
    "!git clone https://github.com/Plachtaa/seed-vc.git\n",
    "print(\"‚úÖ Seed-VC cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%cd seed-vc\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"‚úÖ Seed-VC dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model (if link works)\n",
    "!mkdir -p pretrained_models\n",
    "!wget -q https://huggingface.co/Plachtaa/seed-vc/resolve/main/DiT_seed_v2_uvit_whisper_small_wavenet_bigvgan_pruned.pth -O pretrained_models/model.pth 2>/dev/null || echo \"Download may require manual intervention - check Seed-VC repo\"\n",
    "print(\"Model download attempted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Seed-VC (manual testing if API not available)\n",
    "print(\"\\nüìù Seed-VC Testing:\")\n",
    "print(\"If automated testing fails, check seed-vc/README.md for manual inference instructions\")\n",
    "print(\"\\nModel location: seed-vc/pretrained_models/\")\n",
    "print(\"Test audio: ../test_audio/\")\n",
    "\n",
    "# Add your testing code here based on Seed-VC documentation\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 2: RVC (Most Popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone RVC\n",
    "!git clone https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.git\n",
    "print(\"‚úÖ RVC cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install RVC\n",
    "%cd Retrieval-based-Voice-Conversion-WebUI\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"‚úÖ RVC dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RVC models\n",
    "!python download_models.py 2>/dev/null || echo \"Check RVC repo for model download instructions\"\n",
    "print(\"Model download attempted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RVC\n",
    "print(\"\\nüìù RVC Testing:\")\n",
    "print(\"RVC typically requires:\")\n",
    "print(\"1. Pretrained models in assets/\")\n",
    "print(\"2. Training on your voice samples OR using pretrained voice models\")\n",
    "print(\"3. Check RVC-Project/Retrieval-based-Voice-Conversion-WebUI for detailed docs\")\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 3: GPT-SoVITS (Best Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone GPT-SoVITS\n",
    "!git clone https://github.com/RVC-Boss/GPT-SoVITS.git\n",
    "print(\"‚úÖ GPT-SoVITS cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GPT-SoVITS\n",
    "%cd GPT-SoVITS\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"‚úÖ GPT-SoVITS dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models (this takes time - 5-10 minutes)\n",
    "!python download_models.py 2>/dev/null || echo \"Check GPT-SoVITS repo for manual download\"\n",
    "print(\"Model download attempted (this may take 10+ minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPT-SoVITS\n",
    "print(\"\\nüìù GPT-SoVITS Testing:\")\n",
    "print(\"GPT-SoVITS requires:\")\n",
    "print(\"1. Pretrained models (GPT + SoVITS)\")\n",
    "print(\"2. Reference audio (5s-1min of target voice)\")\n",
    "print(\"3. Check RVC-Boss/GPT-SoVITS for API documentation\")\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ Completed:\")\n",
    "print(\"  - GPU verification\")\n",
    "print(\"  - Repository cloning\")\n",
    "print(\"  - Dependency installation\")\n",
    "print(\"  - Test audio generation\")\n",
    "\n",
    "print(\"\\nüìù Next Steps for Manual Testing:\")\n",
    "print(\"\\n1. Seed-VC:\")\n",
    "print(\"   - Navigate to seed-vc/\")\n",
    "print(\"   - Read README.md\")\n",
    "print(\"   - Run inference examples\")\n",
    "\n",
    "print(\"\\n2. RVC:\")\n",
    "print(\"   - Navigate to Retrieval-based-Voice-Conversion-WebUI/\")\n",
    "print(\"   - Follow inference_main.py examples\")\n",
    "print(\"   - Or use WebUI: python infer-web.py\")\n",
    "\n",
    "print(\"\\n3. GPT-SoVITS:\")\n",
    "print(\"   - Navigate to GPT-SoVITS/\")\n",
    "print(\"   - Start API: python api.py\")\n",
    "print(\"   - Or WebUI: python webui.py\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nFor detailed documentation, visit:\")\n",
    "print(\"https://github.com/MuruganR96/VoiceConversion_Survey\")\n",
    "print(\"\\nSee: SERVER_SIDE_GPU_MODELS.md and SERVER_DEPLOYMENT_GUIDE.md\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List what was cloned\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Manual Benchmarking Template\n",
    "\n",
    "Use this code to benchmark any model once you get it running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_model(conversion_function, input_audio, num_runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark any voice conversion function\n",
    "    \n",
    "    Args:\n",
    "        conversion_function: Your model's inference function\n",
    "        input_audio: Input audio array or path\n",
    "        num_runs: Number of runs to average\n",
    "    \"\"\"\n",
    "    \n",
    "    # Warmup\n",
    "    _ = conversion_function(input_audio)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    times = []\n",
    "    gpu_mem = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        output = conversion_function(input_audio)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        times.append((end - start) * 1000)  # ms\n",
    "        gpu_mem.append(torch.cuda.max_memory_allocated() / 1e9)  # GB\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"BENCHMARK RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Latency: {np.mean(times):.2f} ¬± {np.std(times):.2f} ms\")\n",
    "    print(f\"GPU Memory: {np.max(gpu_mem):.2f} GB (peak)\")\n",
    "    print(f\"GPU Memory: {np.mean(gpu_mem):.2f} GB (average)\")\n",
    "    print(f\"Real-time Factor: {np.mean(times) / 3000:.3f}\")  # Assuming 3s audio\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'latency_ms': np.mean(times),\n",
    "        'latency_std': np.std(times),\n",
    "        'gpu_memory_gb': np.max(gpu_mem),\n",
    "        'rtf': np.mean(times) / 3000\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# results = benchmark_model(your_model.convert, 'test_audio/male.wav')\n",
    "\n",
    "print(\"‚úÖ Benchmarking template ready\")\n",
    "print(\"Use benchmark_model(your_function, input_audio) to measure performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Save Your Results\n",
    "\n",
    "After testing, document your findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for saving results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU',\n",
    "    'models': {\n",
    "        'Seed-VC': {\n",
    "            'tested': False,  # Change to True if you tested it\n",
    "            'latency_ms': 0,  # Fill in your results\n",
    "            'gpu_memory_gb': 0,\n",
    "            'notes': 'Add your observations here'\n",
    "        },\n",
    "        'RVC': {\n",
    "            'tested': False,\n",
    "            'latency_ms': 0,\n",
    "            'gpu_memory_gb': 0,\n",
    "            'notes': ''\n",
    "        },\n",
    "        'GPT-SoVITS': {\n",
    "            'tested': False,\n",
    "            'latency_ms': 0,\n",
    "            'gpu_memory_gb': 0,\n",
    "            'notes': ''\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open('results/my_test_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Results template created: results/my_test_results.json\")\n",
    "print(\"Edit this file with your actual test results\")\n",
    "\n",
    "# Download results\n",
    "from google.colab import files\n",
    "files.download('results/my_test_results.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
